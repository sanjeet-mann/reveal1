<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Measuring Availability</title>

		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/sanjeet.css">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css">
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
			  <section>
			    <p><img data-src="img/measuring-availability-title.png" alt="Screen shot of survey spreadsheet" /></p>
				<h1>Measuring Electronic Resource Availability</h1>
			    <p>Sanjeet Mann<br />
			      University of Redlands<br />
			      SCELC Research Day<br />
			      March 5, 2013</p>
			    <aside class="notes">
			      <p>This is the online, archived version of my presentation given March 5, 2013 at SCELC Research Day, Loyola Marymount University.</p>
			      </aside>
			  </section>
			  <section>
			  <h2>Armacost Library infrastructure</h2>
			  <p><img style="float:left;" data-src="img/armacost-library-infrastructure.png" /></p>
			  <p><ul>
			  <li>ILLIAD interlibrary loan system</li>
			  <li>Full text targets (databases, ejournals): 79,757 unique titles</li>
			  <li>Serials Solutions 360 Link</li>
			  <li>A&I databases: 77,000 titles indexed</li>
			  <li>Innovative Interfaces catalog/proxy: 263,575 book/serial titles</li>
			  </ul>
			  </p>
			  <aside class="notes">
			  <p>This diagram presents an overview of Armacost Library’s e-resource discovery infrastructure. Five systems (proxy server, source database, knowledge base/link resolver, target database and ILL system) must work together using common standards for students and faculty to be able to discover full text.​</p>
			  </aside>
			    </section>
			  <section>
			  <h2>Why do electronic resource errors matter?</h2>
			  <p><ul>
			  <li>Costs</li>
			  <li>Frustrated expectations</li>
			  <li>Undermined confidence</li>
			  <li>Complicated instruction</li>
			  </ul></p>
			  <aside class="notes">
			  <p>Electronic resource errors cost libraries in terms of unrealized value on paid-for content that cannot be accessed, and in terms of staff time spent on troubleshooting. Unnecessary ILL requests also add staff costs and IFM/copyright charges.​ Errors frustrate student and faculty expectations and undermine library staff confidence in the accuracy of their own systems for day-to-day use. Scarce physical and fiscal resources are already compelling libraries to justify their relevance to their campuses; unavailable e-resources only fuel skeptics’ concerns. ​Errors also require instruction librarians to take precious course time away from higher-order thinking skills to explain technical workarounds and search mechanics in greater detail. ​</p>
			  </aside>
			    </section>
			  <section>
			    <h2>Research question</h2>
				<p>"How often does full text linking work?"</p>
			    <aside class="notes">
			      <p> My research study asks the question, how often can Armacost Library users get to the full text of sources they find in abstracting and indexing databases? My study includes, but is not limited to, investigation of OpenURL linking.​ Availability is defined as the ability of library users to get the full text of an item they seek, whether from the library’s electronic collection, physical collection, or from another library through ILL. All items are either available or unavailable due to an error.</p>
			      </aside>
			  </section>
			  <section>
			    <h2>Availability studies</h2>
			    <p style="float:left;"><img data-src="img/gaskill.png" alt="Excerpt from Gaskill's availability study" /></p>
				<p><ul>
				<li>Sample of items</li>
				<li>Available? Yes/No Error?</li>
				<li>Order encountered</li>
				<li>Probabilities</li>
				<li>Prioritize fixes</li>
			    <aside class="notes">
			      <p>Availability studies are a systems analysis research method designed to find out why libraries are unavailable to supply materials to readers, and prioritize troubleshooting efforts. The method was first used in an academic library in 1934 (Gaskill). Investigators generate a sample of items and attempt to retrieve them from the stacks, or download them online. All unavailable items are classified according to the reason why they could not be obtained. Problems can be sorted in the order that a student would encounter them, and assigned probabilities of occurring based on their frequency in the sample. Ideally, librarians would then fix the most frequent problems first.</p>
			      </aside>
			  </section>
			  <section>
			    <h2>Development of the availability technique</h2>
				<p style="float:left;"><img data-src="img/availability-branching.png" /></p>
				<p><ul>
				<li>Print material availability: Card catalog user surveys (Reviewed in Mansbridge 1986, Nisonger 2007)</li>
				<li>Linear sequence (De Prospo 1973)</li>
				<li>Branching model (Kantor 1976)</li>
				<li>Applied to e-resources: 500 articles from 50 high impact journals (Nisonger 2009)</li>
				</ul></p>
			    <aside class="notes">
			      <p>Nisonger and Mansbridge’s review articles give a succinct overview of the availability technique and findings from numerous studies. De Prospo, Kantor and Nisonger have also contributed significantly to our knowledge of this research method.​</p>
			    </aside>
			  </section>
			  <section>
			  <h2>OpenURL performance</h2>
			  <p style="float:left;"><img data-src="img/availability-csun.png" /></p>
			  <p><ul>
			  <li>OpenURL-based reasons for availability error (Wakimoto et al 1998)</li>
			  <li>"Digging into the Data" on link resolver failure (Trainor and Price 2010)</li>
			  <li>NISO initiatives: KBART, IOTA, PIE-J (Chandler et al 2011, Glasser 2012, Kasprowski 2012)</li>
				</p>
				<aside class="notes">
				<p>In addition to the literature on availability studies, research on OpenURL performance was also relevant to my study. These investigations focus on one source of error – the library’s knowledge base. Researchers tested samples of OpenURL links to determine proportions of available and erroneous items. Problems frequently involved the metadata “supply chain” linking publishers, database vendors and knowledge base providers. Several NISO initiatives have sought to improve the quality of e-resource metadata to reduce the frequency of metadata-related errors. ​</p>
				</aside>
			</section>
			<section>
			<h2>Usability studies focusing on e-resources</h2>
			<p style="float:left;"><img data-src="img/availability-bgsu.png" /></p>
			<p><ul>
			<li>Database link pages (Fry 2011, Ponsford et al 2011b)</li>
			<li>Resolver menus (O'Neill 2009, Imler & Eichelberger 2011, Ponsford et al 2011a)</li>
			<li>Discovery services (Williams & Foster 2011, Fagan et al 2012)</li>
			<li>Entire process (Kress 2011)</li>
			</ul></p>
			<aside class="notes">
			<p>Many library website usability studies have focused on how students access electronic resources. These studies focus on interface design and vocabulary issues that affect electronic resource availability. ​Researchers have used a variety of usability methods, including task protocols and cognitive walkthroughs. Studies have either isolated parts of the library’s online presence, or glanced over the entire process a student would use, as in Kress’s study of the reasons why students might place an unnecessary ILL request for an article contained in a subscribed e-journal. ​</p>
			</aside>
			</section>
			<section>
			<h2>Methodology</h2>
			<p><img data-src="img/availability-methodology.png" /></p>
			<aside class="notes">
			<p>I collected a sample of 400 citations in the following manner:​</p>
			<p><ol>
			<li>I identified 10 academic disciplines where we had an abstracting and indexing database​</li>
			<li>I searched our Libstats reference desk software to find four student reference questions for each discipline (40 questions total)​</li>
			<li>I searched the topics in the corresponding A&I databases and tested the first 10 results of each search for full text access (400 citations total)​</li></ol>
			<p>I did not modify the default sort order or browse to subsequent result screens in order to more accurately simulate student research behavior)​</p>
			</aside>
			</section>
			<section>
			<h2>Link testing</h2>
			<p><img data-src="img/availability-link-testing.png" /></p>
			<aside class="notes">
			<p>For each of the 400 items tested, I recorded bibliographic metadata in an Excel spreadsheet archived at http://works.bepress.com/sanjeet_mann/1/​ I also collected “incoming” (from source A&I database to link resolver, see yellow “find full text” link in screenshot) and “outbound” (from link resolver to target full text database, see red circles in screen shot) OpenURLs for each item and pasted them into the spreadsheet. I categorized each item as available or erroneous. After testing all items, I went back and assigned a category of error to each unavailable item. ​</p>
			</aside>
			</section>
			<section>
			<h2>Error coding</h2>
			<p><ul>
			<li>What is an error?</li>
			<li>Six error categories</li>
			<li>Updated criteria</li>
			<p style="float:right;"><img data-src="img/availability-error-coding.png" /></p>
			<aside class="notes"><p>Error categories require judgment calls on the part of the investigator. Many errors (such as incorrect publisher metadata) are not evident at their point of origin, only detectable by problems that occur later in the retrieval process.​ I developed six error categories roughly matching the five systems involved in e-resource retrieval. The next seven slides present an overview of the categories and examples of common errors of each type.​ The availability and openURL-testing literature contains some discussion of what constitutes an unavailable or erroneous resource. I chose to treat ILL requests as a normal part of the retrieval process, rather than as a failure of the library to obtain all items a user might need (something which is no longer possible even for libraries with the most comprehensive collection development policies) I also specified that each item must link directly to a screen providing HTML or PDF full text; screens such as the one pictured here where the link leads to a list of items could confuse students, so I counted it as an error.​</p>
			</aside>
			</section>
			<section>
			<h2>Armacost Library failure points: P vs O</h2>
			<p><img data-src="img/availability-failure-po.png" /></p>
			<aside class="notes">
			<p>A side-by-side comparison of causes for error in the print and online environments demonstrates the additional complexity of conducting research in an online environment. ​</p>
			</aside>
			</section>
			<section>
			<h2>Error details 1: Proxy errors</h2>
			<p style="float:left;"><img data-src="availability-error-1.png" alt="Proxy error" /></p>
			<p><ul>
			<li>Domain missing from forward table</li>
			<li>Domain missing from SSL certificate</li>
			<li>Timeouts trying to establish connection</li>
			</ul>
			</p>
			<aside class="notes">
			<p>Failure can be localized to the proxy server because the full text target database’s domain is missing from the proxy server forward table, because the proxy server SSL certificate does not contain the full text target database domain, or because the proxy server slowed the connection significantly, causing the web browser to time out. User logins are another source of error (not tested in this study)​</p>
			</aside>
			</section>
			<section>
			<h2>Error details 2: Source errors</h2>
			<p class="float:left;"><img data-src="img/availability-error-2.png" alt="Source error" /></p>
			<p><ul>
			<li>Missing metadata</li>
			<li>Erroneous metadata (e.g. rft.date=0001-01-01)</li>
			</ul></p>
			<aside class="notes">
			<p>The source A&I database can cause problems due to its interface design (not tested in this study) or because metadata are missing or erroneous. This can cause the link resolver to fail or delay the processing of ILL requests. Our ILL staff frequently need to verify requests with duplicate information in the article and journal title fields or nonsensical dates (e.g. “0001”). Libraries that have configured their ILL system to automatically send requests (e.g. ILLIAD Direct Request) will experience slower performance as erroneous requests are flagged by the system for human intervention.​</p>
			</aside>
			</section>
			<section>
			<h2>Error details 3: Knowledge base errors</h2>
			<p style="float:left;"><img data-src="img/availability-error-3.png" alt="Knowledge base error" /></p>
			<p><ul>
			<li>Title not selected in knowledge base</li>
			<li>Title selected, but in poorly chosen collection</li>
			<li>Knowledge base holdings do not reflect access entitlement (embargo, back issues, etc)</li>
			</ul></p>
			<aside class="notes">
			<p>Library staff are responsible for selecting titles and collections in knowledge bases that reflect their subscription entitlements. Errors occur if the entire title, or the starting and ending range of the library’s holdings of that title, are either selected when they shouldn’t be (“false positive” error) or not selected when they should be (“false negative” error). Sometimes the same title is listed in multiple collections; library staff must choose the collection with the most complete metadata or risk errors such as the missing article-level link illustrated here (the “SCELC Wiley-Blackwell Collection” lacked information necessary to achieve article level linking throughout the collection, while a different collection with the same titles contained that information)​ Publishers and knowledge base vendors can also contribute to problems at this stage, when a publisher does not notify the knowledge base vendor in a timely manner of publication changes, or when a knowledge base vendor does not accurately reflect the publisher’s embargo or other information pertaining to access. ​</p>
			</aside>
			</section>
			<section>
			<h2>Error details 4: Link resolver error</h2>
			<p style="float:left;"><img data-src="img/availability-error-4.png" alt="Link resolver error" /></p>
			<p><ul>
			<li>Confusion between two similar titles</li>
			<li>Unusual OpenURL syntax</li>
			</ul></p>
			<aside class="notes">
			<p>Link resolvers and target databases contributed relatively few errors in my study. Most link resolver errors involved a failure to draw a match between the requested citation and the item in the target resource. This could be due to idiosyncratic metadata or even to variation in libraries’ cataloging practices. In this example, an article from Costerus, a journal not held online, was then run as a catalog title search, which matched on an issue that had been cataloged as a serial monograph. (This problem is likely incomprehensible to our undergraduates)​</p>
			</aside>
			</section>
			<section>
			<h2>Error details 5: Target errors</h2>
			<p style="float:left;"><img data-src="img/availability-error-5.png" alt="Target database error" /></p>
			<p><ul>
			<li>Content not loaded (supplement, embargo)</li>
			<li>Records concatenated from full text and non-full-text databases</li>
			<li>Server downtime</li>
			</ul></p>
			<aside class="notes">
			<p>Target databases most commonly generated errors because of missing content (either because their publisher agreement forbids loading that content or because they had not notified the link resolver of an embargo). ​Interface issues represent another source of error not tested in my study. One provider’s tendency to concatenate records for the same item from multiple databases (one containing full text, one containing only an abstract) created problems when the full text record was consistently “hidden” in favor of the abstract-only record (which was consistently targeted by the link resolver)​</p>
			</aside>
			</section>
			<section>
			<h2>Error details 6: ILLIAD errors</h2>
			<p style="float: left;"><img data-src="img/availability-error-6.png" alt="ILL error" /></p>
			<p><ul>
			<li>Unicode metadata not displayed properly</li>
			<li>rft.title used for both book title and article title (affects chapters and dissertations)</li>
			</ul></p>
			<aside class="notes">
			<p>Many errors manifested at the point of submitting an ILL request. Articles with foreign-language characters in the title did not display properly because the then-current version of ILLIAD did not support Unicode. (A subsequent upgrade fixed the problem). Also, when ILLIAD received an OpenURL that only used rft.title, it listed the field twice, in both journal name and article name. Our ILL staff frequently referred these issues to me because they were not sure which was the journal title (used to select the correct OCLC record to request)​</p>
			</aside>
			</section>
			
			
			</div>
		</div>

		<script src="dist/reveal.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
			    hash: true,

				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes ]
			});
		</script>
	</body>
</html>
