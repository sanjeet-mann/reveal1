<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Lessons Learned</title>

		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/sanjeet.css">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css">
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
			  <section>
			    <p><img data-src="img/measuring-availability-title.png" alt="Screen shot of survey spreadsheet" /></p>
				<h1>Measuring Electronic Resource Availability</h1>
			    <p>Sanjeet Mann<br />
			      University of Redlands<br />
			      SCELC Research Day<br />
			      March 5, 2013</p>
			    <aside class="notes">
			      <p>This is the online, archived version of my presentation given March 5, 2013 at SCELC Research Day, Loyola Marymount University.</p>
			      </aside>
			  </section>
			  <section>
			  <h2>Armacost Library infrastructure</h2>
			  <p><img style="float:left;" data-src="img/armacost-library-infrastructure.png" /></p>
			  <p><ul>
			  <li>ILLIAD interlibrary loan system</li>
			  <li>Full text targets (databases, ejournals): 79,757 unique titles</li>
			  <li>Serials Solutions 360 Link</li>
			  <li>A&I databases: 77,000 titles indexed</li>
			  <li>Innovative Interfaces catalog/proxy: 263,575 book/serial titles</li>
			  </ul>
			  </p>
			  <aside class="notes">
			  <p>This diagram presents an overview of Armacost Library’s e-resource discovery infrastructure. Five systems (proxy server, source database, knowledge base/link resolver, target database and ILL system) must work together using common standards for students and faculty to be able to discover full text.​</p>
			  </aside>
			    </section>
			  <section>
			  <h2>Why do electronic resource errors matter?</h2>
			  <p><ul>
			  <li>Costs</li>
			  <li>Frustrated expectations</li>
			  <li>Undermined confidence</li>
			  <li>Complicated instruction</li>
			  </ul></p>
			  <aside class="notes">
			  <p>Electronic resource errors cost libraries in terms of unrealized value on paid-for content that cannot be accessed, and in terms of staff time spent on troubleshooting. Unnecessary ILL requests also add staff costs and IFM/copyright charges.​ Errors frustrate student and faculty expectations and undermine library staff confidence in the accuracy of their own systems for day-to-day use. Scarce physical and fiscal resources are already compelling libraries to justify their relevance to their campuses; unavailable e-resources only fuel skeptics’ concerns. ​Errors also require instruction librarians to take precious course time away from higher-order thinking skills to explain technical workarounds and search mechanics in greater detail. ​</p>
			  </aside>
			    </section>
			  <section>
			    <h2>Research question</h2>
				<p>"How often does full text linking work?"</p>
			    <aside class="notes">
			      <p> My research study asks the question, how often can Armacost Library users get to the full text of sources they find in abstracting and indexing databases? My study includes, but is not limited to, investigation of OpenURL linking.​ Availability is defined as the ability of library users to get the full text of an item they seek, whether from the library’s electronic collection, physical collection, or from another library through ILL. All items are either available or unavailable due to an error.</p>
			      </aside>
			  </section>
			  <section>
			    <h2>Availability studies</h2>
			    <p><img data-src="img/gaskill.png" alt="Excerpt from Gaskill's availability study" /></p>
				<p><ul>
				<li>Sample of items</li>
				<li>Available? Yes/No Error?</li>
				<li>Order encountered</li>
				<li>Probabilities</li>
				<li>Prioritize fixes</li>
			    <aside class="notes">
			      <p>Availability studies are a systems analysis research method designed to find out why libraries are unavailable to supply materials to readers, and prioritize troubleshooting efforts. The method was first used in an academic library in 1934 (Gaskill). Investigators generate a sample of items and attempt to retrieve them from the stacks, or download them online. All unavailable items are classified according to the reason why they could not be obtained. Problems can be sorted in the order that a student would encounter them, and assigned probabilities of occurring based on their frequency in the sample. Ideally, librarians would then fix the most frequent problems first.</p>
			      </aside>
			  </section>
			  <section>
			    <h2>Development of the availability technique</h2>
				<p><img style="float:left;" data-src="img/availability-branching.png" /></p>
				<p><ul>
				<li>Print material availability: Card catalog user surveys (Reviewed in Mansbridge 1986, Nisonger 2007)</li>
				<li>Linear sequence (De Prospo 1973)</li>
				<li>Branching model (Kantor 1976)</li>
				<li>Applied to e-resources: 500 articles from 50 high impact journals (Nisonger 2009)</li>
				</ul></p>
			    <aside class="notes">
			      <p>Nisonger and Mansbridge’s review articles give a succinct overview of the availability technique and findings from numerous studies. De Prospo, Kantor and Nisonger have also contributed significantly to our knowledge of this research method.​</p>
			    </aside>
			  </section>
			  <section>
			  <h2>OpenURL performance</h2>
			  <p><img style="float:left;" data-src="img/availability-csun" /></p>
			  <p><ul>
			  <li>OpenURL-based reasons for availability error (Wakimoto et al 1998)</li>
			  <li>"Digging into the Data" on link resolver failure (Trainor and Price 2010)</li>
			  <li>NISO initiatives: KBART, IOTA, PIE-J (Chandler et al 2011, Glasser 2012, Kasprowski 2012)</li>
				</p>
				<aside class="notes">
				<p>In addition to the literature on availability studies, research on OpenURL performance was also relevant to my study. These investigations focus on one source of error – the library’s knowledge base. Researchers tested samples of OpenURL links to determine proportions of available and erroneous items. Problems frequently involved the metadata “supply chain” linking publishers, database vendors and knowledge base providers. Several NISO initiatives have sought to improve the quality of e-resource metadata to reduce the frequency of metadata-related errors. ​</p>
				</aside>
			</section>
			<section>
			<h2>Usability studies focusing on e-resources</h2>
			<p style="float:left;" data-src="img/availability-bgsu" /></p>
			<p><ul>
			<li>Database link pages (Fry 2011, Ponsford et al 2011b)</li>
			<li>Resolver menus (O'Neill 2009, Imler & Eichelberger 2011, Ponsford et al 2011a)</li>
			<li>Discovery services (Williams & Foster 2011, Fagan et al 2012)</li>
			<li>Entire process (Kress 2011)</li>
			</ul></p>
			<aside class="notes">
			<p>Many library website usability studies have focused on how students access electronic resources. These studies focus on interface design and vocabulary issues that affect electronic resource availability. ​Researchers have used a variety of usability methods, including task protocols and cognitive walkthroughs. Studies have either isolated parts of the library’s online presence, or glanced over the entire process a student would use, as in Kress’s study of the reasons why students might place an unnecessary ILL request for an article contained in a subscribed e-journal. ​</p>
			</aside>
			</section>
			<section>
			<h2>Methodology</h2>
			<p><img data-src="img/availability-methodology" /></p>
			<aside class="notes">
			<p>I collected a sample of 400 citations in the following manner:​</p>
			<p><ol>
			<li>I identified 10 academic disciplines where we had an abstracting and indexing database​</li>
			<li>I searched our Libstats reference desk software to find four student reference questions for each discipline (40 questions total)​</li>
			<li>I searched the topics in the corresponding A&I databases and tested the first 10 results of each search for full text access (400 citations total)​</li>
			<p>I did not modify the default sort order or browse to subsequent result screens in order to more accurately simulate student research behavior)​</p>
			</aside>
			</section>
			<section>
			<h2>Link testing</h2>
			<p><img data-src="img/availability-link-testing" /></p>
			<aside class="notes">
			<p>For each of the 400 items tested, I recorded bibliographic metadata in an Excel spreadsheet archived at http://works.bepress.com/sanjeet_mann/1/​ I also collected “incoming” (from source A&I database to link resolver, see yellow “find full text” link in screenshot) and “outbound” (from link resolver to target full text database, see red circles in screen shot) OpenURLs for each item and pasted them into the spreadsheet. I categorized each item as available or erroneous. After testing all items, I went back and assigned a category of error to each unavailable item. ​</p>
			</aside>
			</section>
			<section>
			<h2>Error coding</h2>
			<p><ul>
			<li>What is an error?</li>
			<li>Six error categories</li>
			<li>Updated criteria</li>
			<p style="float:right;"><img data-src="img/availability-error-coding" /></p>
			<aside class="notes">Error categories require judgment calls on the part of the investigator. Many errors (such as incorrect publisher metadata) are not evident at their point of origin, only detectable by problems that occur later in the retrieval process.​ I developed six error categories roughly matching the five systems involved in e-resource retrieval. The next seven slides present an overview of the categories and examples of common errors of each type.​ The availability and openURL-testing literature contains some discussion of what constitutes an unavailable or erroneous resource. I chose to treat ILL requests as a normal part of the retrieval process, rather than as a failure of the library to obtain all items a user might need (something which is no longer possible even for libraries with the most comprehensive collection development policies) I also specified that each item must link directly to a screen providing HTML or PDF full text; screens such as the one pictured here where the link leads to a list of items could confuse students, so I counted it as an error.​</p>
			</aside>
			</section>
			
			</div>
		</div>

		<script src="dist/reveal.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
			    hash: true,

				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes ]
			});
		</script>
	</body>
</html>
