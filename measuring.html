<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Measuring Availability</title>

		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/sanjeet.css">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css">
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
			  <section>
			    <p><img data-src="img/measuring-availability-title.png" style="max-width:50%" alt="Screen shot of survey spreadsheet" /></p>
				<h1>Measuring Electronic Resource Availability</h1>
			    <p>Sanjeet Mann<br />
			      University of Redlands<br />
			      SCELC Research Day<br />
			      March 5, 2013</p>
			    <aside class="notes">
			      <p>This is the online, archived version of my presentation given March 5, 2013 at SCELC Research Day, Loyola Marymount University.</p>
			      </aside>
			  </section>
			  <section>
			  <h2>Armacost Library infrastructure</h2>
			  <div class="column-left">
			  <p><img data-src="img/armacost-library-infrastructure.png" /></p>
			  </div>
			  <div class="column-right">
			  <p><ul>
			  <li>ILLIAD interlibrary loan system</li>
			  <li>Full text targets (databases, ejournals): 79,757 unique titles</li>
			  <li>Serials Solutions 360 Link</li>
			  <li>A&I databases: 77,000 titles indexed</li>
			  <li>Innovative Interfaces catalog/proxy: 263,575 book/serial titles</li>
			  </ul>
			  </p>
			  </div>
			  <aside class="notes">
			  <p>This diagram presents an overview of Armacost Library’s e-resource discovery infrastructure. Five systems (proxy server, source database, knowledge base/link resolver, target database and ILL system) must work together using common standards for students and faculty to be able to discover full text.​</p>
			  </aside>
			    </section>
			  <section>
			  <h2>Why do electronic resource errors matter?</h2>
			  <p><ul>
			  <li>Costs</li>
			  <li>Frustrated expectations</li>
			  <li>Undermined confidence</li>
			  <li>Complicated instruction</li>
			  </ul></p>
			  <aside class="notes">
			  <p>Electronic resource errors cost libraries in terms of unrealized value on paid-for content that cannot be accessed, and in terms of staff time spent on troubleshooting. Unnecessary ILL requests also add staff costs and IFM/copyright charges.​ Errors frustrate student and faculty expectations and undermine library staff confidence in the accuracy of their own systems for day-to-day use. Scarce physical and fiscal resources are already compelling libraries to justify their relevance to their campuses; unavailable e-resources only fuel skeptics’ concerns. ​Errors also require instruction librarians to take precious course time away from higher-order thinking skills to explain technical workarounds and search mechanics in greater detail. ​</p>
			  </aside>
			    </section>
			  <section>
			    <h2>Research question</h2>
				<p>"How often does full text linking work?"</p>
			    <aside class="notes">
			      <p> My research study asks the question, how often can Armacost Library users get to the full text of sources they find in abstracting and indexing databases? My study includes, but is not limited to, investigation of OpenURL linking.​ Availability is defined as the ability of library users to get the full text of an item they seek, whether from the library’s electronic collection, physical collection, or from another library through ILL. All items are either available or unavailable due to an error.</p>
			      </aside>
			  </section>
			  <section>
			    <h2>Availability studies</h2>
				<div class="column-left">
			    <p><img data-src="img/gaskill.png" alt="Excerpt from Gaskill's availability study" /></p>
				</div>
				<div class="column-right">
				<p><ul>
				<li>Sample of items</li>
				<li>Available? Yes/No Error?</li>
				<li>Order encountered</li>
				<li>Probabilities</li>
				<li>Prioritize fixes</li>
				</ul></p></div>
			    <aside class="notes">
			      <p>Availability studies are a systems analysis research method designed to find out why libraries are unavailable to supply materials to readers, and prioritize troubleshooting efforts. The method was first used in an academic library in 1934 (Gaskill). Investigators generate a sample of items and attempt to retrieve them from the stacks, or download them online. All unavailable items are classified according to the reason why they could not be obtained. Problems can be sorted in the order that a student would encounter them, and assigned probabilities of occurring based on their frequency in the sample. Ideally, librarians would then fix the most frequent problems first.</p>
			      </aside>
			  </section>
			  <section>
			    <h2>Development of the availability technique</h2>
				<div class="column-left">
				<p><img data-src="img/availability-branching.png" /></p>
				</div>
				<div class="column-right">
				<p><ul>
				<li>Print material availability: Card catalog user surveys (Reviewed in Mansbridge 1986, Nisonger 2007)</li>
				<li>Linear sequence (De Prospo 1973)</li>
				<li>Branching model (Kantor 1976)</li>
				<li>Applied to e-resources: 500 articles from 50 high impact journals (Nisonger 2009)</li>
				</ul></p></div>
			    <aside class="notes">
			      <p>Nisonger and Mansbridge’s review articles give a succinct overview of the availability technique and findings from numerous studies. De Prospo, Kantor and Nisonger have also contributed significantly to our knowledge of this research method.​</p>
			    </aside>
			  </section>
			  <section>
			  <h2>OpenURL performance</h2>
			  <div class="column-left">
			  <p><img data-src="img/availability-csun.png" /></p>
			  </div>
			  <div class="column-right">
			  <p><ul>
			  <li>OpenURL-based reasons for availability error (Wakimoto et al 1998)</li>
			  <li>"Digging into the Data" on link resolver failure (Trainor and Price 2010)</li>
			  <li>NISO initiatives: KBART, IOTA, PIE-J (Chandler et al 2011, Glasser 2012, Kasprowski 2012)</li>
			  </ul>
				</p>
				</div>
				<aside class="notes">
				<p>In addition to the literature on availability studies, research on OpenURL performance was also relevant to my study. These investigations focus on one source of error – the library’s knowledge base. Researchers tested samples of OpenURL links to determine proportions of available and erroneous items. Problems frequently involved the metadata “supply chain” linking publishers, database vendors and knowledge base providers. Several NISO initiatives have sought to improve the quality of e-resource metadata to reduce the frequency of metadata-related errors. ​</p>
				</aside>
			</section>
			<section>
			<h2>Usability studies focusing on e-resources</h2>
			<div class="column-left">
			<p><img data-src="img/availability-bgsu.png" /></p>
			</div>
			<div class="column-right">
			<p><ul>
			<li>Database link pages (Fry 2011, Ponsford et al 2011b)</li>
			<li>Resolver menus (O'Neill 2009, Imler & Eichelberger 2011, Ponsford et al 2011a)</li>
			<li>Discovery services (Williams & Foster 2011, Fagan et al 2012)</li>
			<li>Entire process (Kress 2011)</li>
			</ul></p></div>
			<aside class="notes">
			<p>Many library website usability studies have focused on how students access electronic resources. These studies focus on interface design and vocabulary issues that affect electronic resource availability. ​Researchers have used a variety of usability methods, including task protocols and cognitive walkthroughs. Studies have either isolated parts of the library’s online presence, or glanced over the entire process a student would use, as in Kress’s study of the reasons why students might place an unnecessary ILL request for an article contained in a subscribed e-journal. ​</p>
			</aside>
			</section>
			<section>
			<h2>Methodology</h2>
			<p><img data-src="img/availability-methodology.png" /></p>
			<aside class="notes">
			<p>I collected a sample of 400 citations in the following manner:​</p>
			<p><ol>
			<li>I identified 10 academic disciplines where we had an abstracting and indexing database​</li>
			<li>I searched our Libstats reference desk software to find four student reference questions for each discipline (40 questions total)​</li>
			<li>I searched the topics in the corresponding A&I databases and tested the first 10 results of each search for full text access (400 citations total)​</li></ol>
			<p>I did not modify the default sort order or browse to subsequent result screens in order to more accurately simulate student research behavior)​</p>
			</aside>
			</section>
			<section>
			<h2>Link testing</h2>
			<p><img data-src="img/availability-link-testing.png" /></p>
			<aside class="notes">
			<p>For each of the 400 items tested, I recorded bibliographic metadata in an Excel spreadsheet archived at http://works.bepress.com/sanjeet_mann/1/​ I also collected “incoming” (from source A&I database to link resolver, see yellow “find full text” link in screenshot) and “outbound” (from link resolver to target full text database, see red circles in screen shot) OpenURLs for each item and pasted them into the spreadsheet. I categorized each item as available or erroneous. After testing all items, I went back and assigned a category of error to each unavailable item. ​</p>
			</aside>
			</section>
			<section>
			<h2>Error coding</h2>
			<div class="column-left">
			<p><img data-src="img/availability-error-coding.png" alt="example of a target database error"/></p></div>
			<div class="column-right">
			<p><ul>
			<li>What is an error?</li>
			<li>Six error categories</li>
			<li>Updated criteria</li>
			</ul></p></div>
			<aside class="notes"><p>Error categories require judgment calls on the part of the investigator. Many errors (such as incorrect publisher metadata) are not evident at their point of origin, only detectable by problems that occur later in the retrieval process.​ I developed six error categories roughly matching the five systems involved in e-resource retrieval. The next seven slides present an overview of the categories and examples of common errors of each type.​ The availability and openURL-testing literature contains some discussion of what constitutes an unavailable or erroneous resource. I chose to treat ILL requests as a normal part of the retrieval process, rather than as a failure of the library to obtain all items a user might need (something which is no longer possible even for libraries with the most comprehensive collection development policies) I also specified that each item must link directly to a screen providing HTML or PDF full text; screens such as the one pictured here where the link leads to a list of items could confuse students, so I counted it as an error.​</p>
			</aside>
			</section>
			<section>
			<h2>Armacost Library failure points: P vs O</h2>
			<p><img data-src="img/availability-failure-po.png" /></p>
			<aside class="notes">
			<p>A side-by-side comparison of causes for error in the print and online environments demonstrates the additional complexity of conducting research in an online environment. ​</p>
			</aside>
			</section>
			<section>
			<h2>Error details 1: Proxy errors</h2>
			<p style="float:left; max-width:49%;"><img data-src="img/availability-error-1.png" alt="Proxy error" /></p>
			<p><ul>
			<li>Domain missing from forward table</li>
			<li>Domain missing from SSL certificate</li>
			<li>Timeouts trying to establish connection</li>
			</ul>
			</p>
			<aside class="notes">
			<p>Failure can be localized to the proxy server because the full text target database’s domain is missing from the proxy server forward table, because the proxy server SSL certificate does not contain the full text target database domain, or because the proxy server slowed the connection significantly, causing the web browser to time out. User logins are another source of error (not tested in this study)​</p>
			</aside>
			</section>
			<section>
			<h2>Error details 2: Source errors</h2>
			<p class="float:left; max-width:49%;"><img data-src="img/availability-error-2.png" alt="Source error" /></p>
			<p class="float:right; max-width:49%;"><ul>
			<li>Missing metadata</li>
			<li>Erroneous metadata (e.g. rft.date=0001-01-01)</li>
			</ul></p>
			<aside class="notes">
			<p>The source A&I database can cause problems due to its interface design (not tested in this study) or because metadata are missing or erroneous. This can cause the link resolver to fail or delay the processing of ILL requests. Our ILL staff frequently need to verify requests with duplicate information in the article and journal title fields or nonsensical dates (e.g. “0001”). Libraries that have configured their ILL system to automatically send requests (e.g. ILLIAD Direct Request) will experience slower performance as erroneous requests are flagged by the system for human intervention.​</p>
			</aside>
			</section>
			<section>
			<h2>Error details 3: Knowledge base errors</h2>
			<p style="float:left; max-width:49%;"><img data-src="img/availability-error-3.png" alt="Knowledge base error" /></p>
			<p style="float:right; max-width:49%;"><ul>
			<li>Title not selected in knowledge base</li>
			<li>Title selected, but in poorly chosen collection</li>
			<li>Knowledge base holdings do not reflect access entitlement (embargo, back issues, etc)</li>
			</ul></p>
			<aside class="notes">
			<p>Library staff are responsible for selecting titles and collections in knowledge bases that reflect their subscription entitlements. Errors occur if the entire title, or the starting and ending range of the library’s holdings of that title, are either selected when they shouldn’t be (“false positive” error) or not selected when they should be (“false negative” error). Sometimes the same title is listed in multiple collections; library staff must choose the collection with the most complete metadata or risk errors such as the missing article-level link illustrated here (the “SCELC Wiley-Blackwell Collection” lacked information necessary to achieve article level linking throughout the collection, while a different collection with the same titles contained that information)​ Publishers and knowledge base vendors can also contribute to problems at this stage, when a publisher does not notify the knowledge base vendor in a timely manner of publication changes, or when a knowledge base vendor does not accurately reflect the publisher’s embargo or other information pertaining to access. ​</p>
			</aside>
			</section>
			<section>
			<h2>Error details 4: Link resolver error</h2>
			<p style="float:left; max-width:49%;"><img data-src="img/availability-error-4.png" alt="Link resolver error" /></p>
			<p><ul>
			<li>Confusion between two similar titles</li>
			<li>Unusual OpenURL syntax</li>
			</ul></p>
			<aside class="notes">
			<p>Link resolvers and target databases contributed relatively few errors in my study. Most link resolver errors involved a failure to draw a match between the requested citation and the item in the target resource. This could be due to idiosyncratic metadata or even to variation in libraries’ cataloging practices. In this example, an article from Costerus, a journal not held online, was then run as a catalog title search, which matched on an issue that had been cataloged as a serial monograph. (This problem is likely incomprehensible to our undergraduates)​</p>
			</aside>
			</section>
			<section>
			<h2>Error details 5: Target errors</h2>
			<p style="float:left; max-width:49%;"><img data-src="img/availability-error-5.png" alt="Target database error" /></p>
			<p style="float:left; max-width:49%;"><ul>
			<li>Content not loaded (supplement, embargo)</li>
			<li>Records concatenated from full text and non-full-text databases</li>
			<li>Server downtime</li>
			</ul></p>
			<aside class="notes">
			<p>Target databases most commonly generated errors because of missing content (either because their publisher agreement forbids loading that content or because they had not notified the link resolver of an embargo). ​Interface issues represent another source of error not tested in my study. One provider’s tendency to concatenate records for the same item from multiple databases (one containing full text, one containing only an abstract) created problems when the full text record was consistently “hidden” in favor of the abstract-only record (which was consistently targeted by the link resolver)​</p>
			</aside>
			</section>
			<section>
			<h2>Error details 6: ILLIAD errors</h2>
			<p style="float: left; max-width:49%;"><img data-src="img/availability-error-6.png" alt="ILL error" /></p>
			<p style="float:right; max-width:49%;"><ul>
			<li>Unicode metadata not displayed properly</li>
			<li>rft.title used for both book title and article title (affects chapters and dissertations)</li>
			</ul></p>
			<aside class="notes">
			<p>Many errors manifested at the point of submitting an ILL request. Articles with foreign-language characters in the title did not display properly because the then-current version of ILLIAD did not support Unicode. (A subsequent upgrade fixed the problem). Also, when ILLIAD received an OpenURL that only used rft.title, it listed the field twice, in both journal name and article name. Our ILL staff frequently referred these issues to me because they were not sure which was the journal title (used to select the correct OCLC record to request)​</p>
			</aside>
			</section>
			<section>
			<h2>Results</h2>
			<p><img data-src="img/availability-results.png" alt="Branching diagram showing 250 of 400 or 62% items were available" /></p>
			<aside class="notes">
			<p>250 of 400 items were available in the library’s electronic collection, physical collection, or by ILL.​ 99 of 400 items were available as local, electronic downloads. ​These sample findings can be generalized to the entire population of all e-resources at Armacost Library with over 95% confidence and +/-5% margin of error.</p>
			<p>Out of every 100 citations, I would expect to find:​</p>
			<p><ul>
			<li>38 errors significant enough to prevent a student from obtaining full text or successfully placing an ILL request​</li>
			<li>34 potentially successful ILL requests​</li>
			<li>2 items available from the physical collection​</li>
			<li>26 full text downloads​</li>
			</ul></p>
			</aside>
			</section>
			<section>
			<h2>Sampling</h2>
			<p>Necessary sample size for a yes/no condition is determined by:</p>
			<p>\[n=p(1-p)(\frac{z_c^2}{E^2})\]</p>
			<div style="float:left; max-width:49%; border-right-width:2px;">
			<p>To use this, you need:</p>
			<p><ul><li>Availability rate from a small pre-test</li>
			<li>Choose acceptable % confidence (95%)</li>
			<li>Choose acceptable margin of error (+/- 5%)</li></ul></p>
			</div>
			<div style="float:right; max-width:49%;">
			<p>Plug values into the formula...</p>
			<p><ul><li>\(p\) = 0.625 (250/400 successes)</li>
			<li>\(1-p\) = 0.375 (150/400 errors)</li>
			<li>\(C\) = 0.95 (95% confidence)</li>
			<li>\(z_c\) = 1.96 (statistical textbook or <a href="http://www.measuringusability.com/pcalcz.php" target="_blank">an online table</a></li>
			<li>\(E\) = 0.05 (5% error)</li></ul></p>
			</div>
			<aside class="notes">
			<p>Full-text availability and the presence of error are yes/no (Bernoulli or Binomial) outcomes. Statistical textbooks (e.g. Brase 1987) give the formula for determining the sample size for a binomial population. You will need to conduct a pre-test first to obtain values for the proportion of successful and unsuccessful outcomes. You can choose confidence (\(c\)) and error (\(E\)) values arbitrarily. The lower the values, the less strong your study, but the easier it is to conduct because you can use a smaller sample. The value \(z_c\) is found in a table online or in a statistical textbook. It is related to your confidence: the higher your confidence, the greater \(z_c\) becomes.​</p>
			</aside>
			</section>
			<section>
			<h2>Confidence</h2>
			<p>Your confidence in a study of a particular sample sizes is given by:</p>
			<p>\[z_c=E\sqrt{\frac{n}{p(1-p)}}\]</p>
			<p>(rewritten from previous formula, plug in values as before)</p>
			<p>Use table of areas under a standard normal curve to convert \(z_c\) to a confidence probability.</p>
			<aside class="notes">
			<p>Rewriting the equation to solve for \(z_c\) gives you this equation, which lets you state your level of confidence in a study of a particular sample size. Look up the \(z_c\) value in the table of standard normal distributions or online to determine the confidence probability. ​Note that small, convenient samples can still obtain a reasonably high confidence probability.​</p>
			</aside>
			</section>
			<section>
			<h2>Discussion</h2>
			<p><img data-src="img/availability-discussion-1.png" alt="Pie charts for source by type and errors by source type" /></p>
			<aside class="notes">
			<p>Most of the sources I tested were articles, but dissertations and book chapters generated a disproportionately great number of errors. ​</p>
			</aside>
			</section>
			<section>
			<h2>Discussion (continued)</h2>
			<p><img data-src="img/availability-discussion-2.png" alt="Pie chart of errors by error type" /></p>
			<aside class="notes">
			<p>Most of my errors occurred at the source database, knowledge base or ILL stages.​</p>
			</aside>
			</section>
			<section>
			<h2>Availability/Error by Discipline</h2>
			<p><img data-src="img/availability-results-discipline.png" alt="bar chart comparing availability by discipline" /></p>
			<aside class="notes">
			<p>There was considerable variation by discipline. The music searches produced results that were rarely downloadable full-text and frequently triggered errors, while history searches frequently led me to a seamless full-text download. ​Several factors could be influencing these results. Different databases may have different metadata standards and publishers and are distributed by different vendors. Some databases index a lot of hard-to-obtain items like conference proceedings, while others mostly consist of journal articles. Some vendors augment their A&I search results with “linked full text” PDFs from another database on the same platform. This type of direct comparison is not useful for assigning responsibility for error, but is interesting for subject librarians who need to know the challenges students in their liaison areas may be facing. ​</p>
			</aside>
			</section>
			<section>
			<h2>Solutions</h2>
			<p><img data-src="img/availability-solutions-forward-table.png" alt="Screen shot of Innopac WAM forward table" /></p>
			<p>Edit proxy forward table</p>
			<aside class="notes">
			<p>I tackled some simpler solutions immediately after finishing my study. I added missing domains to the proxy forward table.</p>
			</aside>
			</section>
			<section>
			<h2>Solutions</h2>
			<p><img data-src="img/availability-solutions-upgrade-illiad.png" alt="ILLIAD request form" /></p>
			<p>Upgrade ILLIAD</p>
			<aside class="notes">
			<p>I upgraded ILLIAD to get Unicode support.</p>
			</aside>
			</section>
			<section>
			<h2>Solutions</h2>
			<p><img data-src="img/availability-solutions-customize-serials-solutions.png" alt="Screen shots of a title-level link resolver result and Serials Solutions screen" /></p>
			<p>Customize Serials Solutions</p>
			<aside class="notes">
			<p>I corrected holdings in Serials Solutions.</p>
			</aside>
			</section>
			<section>
			<h2>Solutions</h2>
			<p><img data-src="img/availability-solutions-simplify.png" alt="Screen shot of new link resolver result screen" /></p>
			<p>Simplify result screen interface and terminology</p>
			<aside class="notes">
			<p>I worked with our web team to make the link resolver result screen easier to understand.</p>
			</aside>
			</section>
			<section>
			<h2>Summary</h2>
			<p><ul>
			<li>400 citations obtained through likely keyword searches of 10 A&I databases</li>
			<ul><li>62.5% availability (98% confidence, +/- 5%)</li>
			<li>24.8% downloadable full text</li></ul>
			<li>Responses include fixing proxy, kb holdings, interfaces, upgrading systems</li>
			<li>Strengths: quant+qual data, very flexible (n=100 allows 85% confidence)</li>
			<li>Weaknesses: Does not account for interface problems, searching, or evaluation by actual users</li>
			</ul>
			</p>
			<aside class="notes">
			<p>Availability studies are a flexible technique to get quantifiable information about students’ access to full text. My study was a “simulated” study because I tested the access myself, rather than using actual library patrons.​</p>
			</aside>
			</section>
			<section>
			<h2>Towards availability testing with live students</h2>
			<p style="float:left; max-width:49%;">
			<img data-src="img/availability-towards-testing-live-students.png" alt="Diagram of e-resource availability model" />
			</p>
			<p style="float:right; max-width:49%;"><ul>
			<li>More barriers</li>
			<ul><li>confusing interfaces</li>
			<li>difficulty formulating searches and evaluating sources</li>
			<li>login errors</li></ul>
			<li>How to test<li>
			<ul><li>cognitive walkthrough + recorded task protocols</li>
			<li>analysis ​informs information literacy and interface design</li></ul>
			<li>Deliverables<li>
			<ul><li>availability %</li>
			<li>branching model</li>
			<li>usability report</li></ul>
			</ul></p>
			<aside class="notes">
			<p>No studies have attempted an electronic resource availability study with library patrons so far. Such a study would add several potential causes for error. The study would need to incorporate usability methods, for example, conducting a cognitive walkthrough of the path Armacost Library users could follow to research a particular topic, then observing students trying to search that same topic. ​</p>
			</aside>
			</section>
			<section>
			<h2>Questions</h2>
			<p><img data-src="img/availability-questions.png" alt="Image of a book with a blank page that reads 'The page could not be found'" /></p>
			<p>Works cited and data set at (dead link)</p>
			<aside class="notes">
			<p>Follow these links to view my works cited list and dataset. Email me with questions at (dead email address)</p>
			</aside>
			</section>
			
			</div>
		</div>

		<script src="dist/reveal.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script src="plugin/math/math.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
			    hash: true,

				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath.KaTeX ]
			});
		</script>
	</body>
</html>
