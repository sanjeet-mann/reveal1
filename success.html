<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Fulltext Success</title>

		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/sanjeet.css">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css">
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
			  <section>
			    <h1>How much do availability studies increase full text success?</h1>
				<p>Sanjeet Mann<br />
			      Arts & Electronic Resources Librarian, University of Redlands<br />
			      SCELC Research Day<br />
			      March 4, 2014</p>
			    <aside class="notes">
			      <p>This is the online, archived version of my presentation given March 4, 2014 at SCELC Research Day at Loyola Marymount University.</p>
				  <p>This presentation builds off of my 2013 SCELC Research Day presentation.</p>
			      </aside>
			  </section>
			  <section>
			  <h2>Availability studies</h2>
			  <p>Defined as a research method replicating the steps that library users take from a citation to full text (or an error)</p>
			  <aside class="notes">
			  <p>Availability studies are a research method that involves replicating the steps that library users take to go from a citation to full text (or an error). They use a sample to approximate the proportion of your entire library collection that is available in full text or generates an error. </p>
			  </aside>
			  </section>
			  <section>
			  <h2>Electronic resource infrastructure</h2>
			  <p><img data-src="img/availability-infrastructure.png" alt="Diagram of electronic resource infrastructure" /></p>
			  <aside class="notes">
			  <p>Availability studies are a systems analysis research method, measuring the performance of the library as a “system” able to deliver full text information. Different researchers have defined the system in slightly different ways. In my research study, I identified six discrete parts. Each of them has to work together for users to get to full text, and each of them is a potential point of failure. </p>
			  </aside>
			  </section>
			  <section>
			  <h2>Example</h2>
			  <p><img data-src="img/availability-example.png" alt="Example of an ILLIAD error" /></p>
			  <aside class="notes">
			  <p>This example illustrates the kind of problems I hope to find and correct through availability research. Source database contributed metadata from a book chapter into an ILLIAD article request form; a unicode error occurred, and an invalid date was provided. This request cannot be filled automatically via Direct Request, and may require significant intervention from a librarian or ILL staff before it can be placed at all. These kinds of problems cost time (and money) to troubleshoot and can be very frustrating for library users and staff alike. My availability research seeks to answer three questions: why do problems like this occur, how often do they happen, and what can we do as librarians to stop these problems from happening again? </p>
			  </aside>
			  </section>
			  <section>
			  <h2>Questions you can answer</h2>
			  <p><ul>
			  <li>Can users get to full text?</li>
			  <li>How often do they get errors? What kind of errors?</li>
			  <li>Should you be satisfied with your knowledge base (kb) vendor?</li>
			  <li>Do you have enough full text in your collection?</li>
			  <li>How often do your users need ILL?</li>
			  <li>Are you teaching users what they need to know to successfully access e-resources?</li>
			  </ul></p>
			  <aside class="notes">
			  <p>Lessons learned from availability studies pertain to other areas of librarianship besides e-resources troubleshooting. You can use study results as evidence to address a variety of questions. </p>
			  </aside>
			  </section>
			  <section>
			  <h2>Availability studies</h2>
			  <p>4 searches x 10 disciplines x 10 results = <strong>400 item sample</strong></p>
			  <p>(estimates overall population with 98% confidence, +/- 5% error)</p>
			  <p><img data-src="img/availability-spreadsheet.png" alt="Screen shot of the availability survey spreadsheet" /></p>
			  <aside class="notes">
			  <p>I created a sample of 400 items by creating database searches based on actual reference questions from our ref desk statistics. I ran 4 searches in each of 10 A&I databases for different disciplines spanning the humanities, social sciences and sciences. I tried to obtain the full text of the first 10 results of each search, recording the results in a spreadsheet. All items are either locally available online, locally available in print, or available from other libraries through ILL. Errors are recorded in the six categories mentioned previously. Using statistical techniques for sampling of binomial (yes/no) variables, I determined that I can be 98% confident that my sample size of 400 items will estimate the population of all items in the library’s collection within +/- 5% margin of error.</p>
			  </aside>
			  </section>
			  <section>
			  <h2>Datasets</h2>
			  <p>2012 dataset at (broken link)</p>
			  <p>2013 dataset at (broken link)</p>
			  <aside class="notes">
			  <p>So far I have conducted three availability studies at Armacost Library. I conducted a pilot project to determine the sample size, and then the initial study of 400 items which I reported on last year at Research Day. After troubleshooting (discussed later) I replicated the same study again in 2013 to see if the local availability and error rates had changed. This was an important step to see if the availability study really had been effective at helping me discover problems. </p>
			  </aside>
			  </section>
			  <section>
			  <h2>Results</h2>
			  <p><img data-src="img/availability-success-results.png" alt="Bar graph comparing 2012 and 2013 results" /></p>
			  <aside class="notes">
			  <p>A comparison of my results from the two studies shows that the proportion of items locally available from Armacost Library’s print and online collections stayed the same, but the error rate dropped from 38% to 13%. Is this significant? </p>
			  </aside>
			  </section>
			  <section>
			  <h2>Z test results</h2>
			  <p><img data-src="img/availability-success-zresults.png" alt="Z test results plotted against a normal curve" /></p>
			  <aside class="notes">
			  <p>Used <i>100 Statistical Tests</i> by Gopal Kanji to determine that I could use the z test for difference of two proportions to see if the change in error rates was significant. The test looks at how much my results differ from the result we would expect if there really was no change in error rates. Z > 1.645 is statistically significant. The reduction in overall error rate is a VERY strong result so I can easily reject H0. No significant change in local availability rate so I would conclude that the troubleshooting steps I took did not make more items locally available. I also used Z tests to examine the change in error rates for each category of error to try to understand why I was or was not effective at fixing each category of error. </p>
			  </aside>
			  </section>


			  

			  


			
			</div>
		</div>

		<script src="dist/reveal.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
			    hash: true,

				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes ]
			});
		</script>
	</body>
</html>
