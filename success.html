<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Fulltext Success</title>

		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/sanjeet.css">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css">
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
			  <section>
			    <h1>How much do availability studies increase full text success?</h1>
				<p>Sanjeet Mann<br />
			      Arts & Electronic Resources Librarian, University of Redlands<br />
			      SCELC Research Day<br />
			      March 4, 2014</p>
			    <aside class="notes">
			      <p>This is the online, archived version of my presentation given March 4, 2014 at SCELC Research Day at Loyola Marymount University.</p>
				  <p>This presentation builds off of my 2013 SCELC Research Day presentation.</p>
			      </aside>
			  </section>
			  <section>
			  <h2>Availability studies</h2>
			  <p>Replicate the steps that library users take from a citation to full text (or an error)</p>
			  <aside class="notes">
			  <p>Availability studies are a research method that involves replicating the steps that library users take to go from a citation to full text (or an error). They use a sample to approximate the proportion of your entire library collection that is available in full text or generates an error. </p>
			  </aside>
			  </section>
			  <section>
			  <h2>Electronic resource infrastructure</h2>
			  <p><img data-src="img/availability-infrastructure.png" alt="Diagram of electronic resource infrastructure" /></p>
			  <aside class="notes">
			  <p>Availability studies are a systems analysis research method, measuring the performance of the library as a “system” able to deliver full text information. Different researchers have defined the system in slightly different ways. In my research study, I identified six discrete parts. Each of them has to work together for users to get to full text, and each of them is a potential point of failure. </p>
			  </aside>
			  </section>
			  <section>
			  <h2>Example</h2>
			  <p><img data-src="img/availability-example.png" alt="Example of an ILLIAD error" /></p>
			  <aside class="notes">
			  <p>This example illustrates the kind of problems I hope to find and correct through availability research. Source database contributed metadata from a book chapter into an ILLIAD article request form; a unicode error occurred, and an invalid date was provided. This request cannot be filled automatically via Direct Request, and may require significant intervention from a librarian or ILL staff before it can be placed at all. These kinds of problems cost time (and money) to troubleshoot and can be very frustrating for library users and staff alike. My availability research seeks to answer three questions: why do problems like this occur, how often do they happen, and what can we do as librarians to stop these problems from happening again? </p>
			  </aside>
			  </section>
			  <section>
			  <h2>Questions you can answer</h2>
			  <p><ul>
			  <li>Can users get to full text?</li>
			  <li>How often do they get errors? What kind of errors?</li>
			  <li>Should you be satisfied with your knowledge base (kb) vendor?</li>
			  <li>Do you have enough full text in your collection?</li>
			  <li>How often do your users need ILL?</li>
			  <li>Are you teaching users what they need to know to successfully access e-resources?</li>
			  </ul></p>
			  <aside class="notes">
			  <p>Lessons learned from availability studies pertain to other areas of librarianship besides e-resources troubleshooting. You can use study results as evidence to address a variety of questions. </p>
			  </aside>
			  </section>
			  <section>
			  <h2>Availability studies</h2>
			  <p>4 searches x 10 disciplines x 10 results = <strong>400 item sample</strong></p>
			  <p>(estimates overall population with 98% confidence, +/- 5% error)</p>
			  <p><img data-src="img/availability-spreadsheet.png" alt="Screen shot of the availability survey spreadsheet" /></p>
			  <aside class="notes">
			  <p>I created a sample of 400 items by creating database searches based on actual reference questions from our ref desk statistics. I ran 4 searches in each of 10 A&I databases for different disciplines spanning the humanities, social sciences and sciences. I tried to obtain the full text of the first 10 results of each search, recording the results in a spreadsheet. All items are either locally available online, locally available in print, or available from other libraries through ILL. Errors are recorded in the six categories mentioned previously. Using statistical techniques for sampling of binomial (yes/no) variables, I determined that I can be 98% confident that my sample size of 400 items will estimate the population of all items in the library’s collection within +/- 5% margin of error.</p>
			  </aside>
			  </section>
			  <section>
			  <h2>Datasets</h2>
			  <p>2012 dataset at (broken link)</p>
			  <p>2013 dataset at (broken link)</p>
			  <aside class="notes">
			  <p>So far I have conducted three availability studies at Armacost Library. I conducted a pilot project to determine the sample size, and then the initial study of 400 items which I reported on last year at Research Day. After troubleshooting (discussed later) I replicated the same study again in 2013 to see if the local availability and error rates had changed. This was an important step to see if the availability study really had been effective at helping me discover problems. </p>
			  </aside>
			  </section>
			  <section>
			  <h2>Results</h2>
			  <p><img data-src="img/availability-success-results.png" alt="Bar graph comparing 2012 and 2013 results" /></p>
			  <aside class="notes">
			  <p>A comparison of my results from the two studies shows that the proportion of items locally available from Armacost Library’s print and online collections stayed the same, but the error rate dropped from 38% to 13%. Is this significant? </p>
			  </aside>
			  </section>
			  <section>
			  <h2>Z test results</h2>
			  <p><img data-src="img/availability-success-zresults.png" alt="Z test results plotted against a normal curve" /></p>
			  <aside class="notes">
			  <p>Used <i>100 Statistical Tests</i> by Gopal Kanji to determine that I could use the z test for difference of two proportions to see if the change in error rates was significant. The test looks at how much my results differ from the result we would expect if there really was no change in error rates. Z > 1.645 is statistically significant. The reduction in overall error rate is a VERY strong result so I can easily reject H0. No significant change in local availability rate so I would conclude that the troubleshooting steps I took did not make more items locally available. I also used Z tests to examine the change in error rates for each category of error to try to understand why I was or was not effective at fixing each category of error. </p>
			  </aside>
			  </section>
			  <section>
			  <h2>Troubleshooting proxy errors</h2>
			  <p><img data-src="img/availability-success-results-proxy.png" alt="Z test results for proxy errors" /></p>
			  <aside class="notes">
			  <p>Proxy errors generally meant the e-journal’s domain was missing from the proxy forward table. Usually these were e-journals with a unique domain. The solution, adding the domain to the forward table, only took a few minutes and was entirely under my control. I had a statistically significant impact on the proxy error rate. </p>
			  </aside>
			  </section>
			  <section>
			  <h2>Troubleshooting source errors</h2>
			  <p><img data-src="img/availability-success-results-source.png" alt="Z test results for source errors" /></p>
			  <aside class="notes">
			  <p>Source errors usually involve OpenURL metadata in the A&I database. There were a lot of these errors (8% of all items in my sample!) Sometimes very difficult to identify because the problem might not manifest until linking into a target database or placing an ILLIAD request. You have to look carefully at the OpenURL string before and after it enters the link resolver to determine if you have a source error. Source errors are also difficult to troubleshoot. I contact the database vendor each time I find an error, and they often need to forward the problem to a different unit, which may need to ask another party to fix the problem. It’s interesting to notice a trend in the 2013 study. In some situations where a library subscribes to both a full text and a citation-only database from the same publisher indexing the same item, the database inserts the PDF from the full text record into the record for the abstracting database, thus bypassing OpenURL linking entirely – and rendering source metadata errors a moot point.</p>
			  </aside>
			  </section>
			  <section>
			  <h2>Troubleshooting KB errors</h2>
			  <p><img data-src="img/availability-success-results-kb.png" alt="Z test results for knowledge base errors" /></p>
			  <aside class="notes">
			  <p>Knowledge base errors usually involve collections that do not resolve down to the article level. In my experience students often assume the item is not available when they are not taken directly to the article so I count this situation as an error that needs to be improved (cf. Trainor and Price 2010 who also categorized browsing for results as an error)  The collections in our knowledgebase for “Freely Available” journals are not centrally managed and only resolve to the title level, so I couldn’t affect the overall error rate.</p>
			  </aside>
			  </section>
			  <section>
			  <h2>Troubleshooting resolver errors</h2>
			  <p><img data-src="img/availability-success-results-resolver.png" alt="Z test results for link resolver errors" /></p>
			  <aside class="notes">
			  <p>Link resolver errors involve problems with the logic used to match items. For example, linking from the resolver result screen to the library catalog by title may match on multiple items or an incorrect item. We fixed problems by adding a prominent link to match on ISSN or ISBN instead. </p>
			  </aside>
			  </section>
			  <section>
			  <h2>Troubleshooting target errors</h2>
			  <p><img data-src="img/availability-success-results-target.png" alt="Z test results for target errors" /></p>
			  <aside class="notes">
			  <p>Target errors could occur because the full text database being linked into does not have the PDF loaded on their site or the site was down when I was testing it. Database vendors improved this category of problems through website redesigns, and by overlaying full text records as mentioned earlier.</p>
			  </aside>
			  </section>
			  <section>
			  <h2>Troubleshooting ILLIAD errors</h2>
			  <p><img data-src="img/availability-succes-results-illiad.png" alt="Z test results for ILLIAD errors" /></p>
			  <aside class="notes">
			  <p>ILLIAD errors improved because OCLC upgraded ILLIAD to support Unicode, and because our web librarian created request forms for more item types and edited the Customization Manager to match OpenURL metadata with the correct field in the correct request form. </p>
			  </aside>
			  </section>
			  <section>
			  <h2>Item types in the sample</h2>
			  <p><img data-src="img/availability-success-item-types.png" alt="Pie chart of item types" /></p>
			  <aside class="notes">
			  <p>Some interesting facts about the samples: 75% articles, 11% gray literature (proceedings and dissertations, ILL only)</p>
			  </aside>
			  </section>
			  <section>
			  <h2>Error rate by discipline</h2>
			  <p><img data-src="img/availability-success-disciplines.png" alt="Bar graph of discipline error rates" /></p>
			  <aside class="notes">
			  <p>Disciplines where the database indexed a lot of gray literature (Music, English, PsycInfo, Math) had very high ILL error rates. Disciplines with article-dominant databases (History, Economics) had fewer errors and more locally available items. Philosophy had an unusually large number of source errors in 2013 study. </p>
			  </aside>
			  </section>




			  

			  


			
			</div>
		</div>

		<script src="dist/reveal.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
			    hash: true,

				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes ]
			});
		</script>
	</body>
</html>
