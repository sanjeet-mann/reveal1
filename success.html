<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Fulltext Success</title>

		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/sanjeet.css">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css">
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
			  <section>
			    <h1>How much do availability studies increase full text success?</h1>
				<p>Sanjeet Mann<br />
			      Arts & Electronic Resources Librarian, University of Redlands<br />
			      SCELC Research Day<br />
			      March 4, 2014</p>
			    <aside class="notes">
			      <p>This is the online, archived version of my presentation given March 4, 2014 at SCELC Research Day at Loyola Marymount University.</p>
				  <p>This presentation builds off of my 2013 SCELC Research Day presentation.</p>
			      </aside>
			  </section>
			  <section>
			  <h2>Availability studies</h2>
			  <p>Replicate the steps that library users take from a citation to full text (or an error)</p>
			  <aside class="notes">
			  <p>Availability studies are a research method that involves replicating the steps that library users take to go from a citation to full text (or an error). They use a sample to approximate the proportion of your entire library collection that is available in full text or generates an error. </p>
			  </aside>
			  </section>
			  <section>
			  <h2>Electronic resource infrastructure</h2>
			  <p><img data-src="img/availability-infrastructure.png" alt="Diagram of electronic resource infrastructure" /></p>
			  <aside class="notes">
			  <p>Availability studies are a systems analysis research method, measuring the performance of the library as a “system” able to deliver full text information. Different researchers have defined the system in slightly different ways. In my research study, I identified six discrete parts. Each of them has to work together for users to get to full text, and each of them is a potential point of failure. </p>
			  </aside>
			  </section>
			  <section>
			  <h2>Example</h2>
			  <p><img data-src="img/availability-example.png" alt="Example of an ILLIAD error" /></p>
			  <aside class="notes">
			  <p>This example illustrates the kind of problems I hope to find and correct through availability research. Source database contributed metadata from a book chapter into an ILLIAD article request form; a unicode error occurred, and an invalid date was provided. This request cannot be filled automatically via Direct Request, and may require significant intervention from a librarian or ILL staff before it can be placed at all. These kinds of problems cost time (and money) to troubleshoot and can be very frustrating for library users and staff alike. My availability research seeks to answer three questions: why do problems like this occur, how often do they happen, and what can we do as librarians to stop these problems from happening again? </p>
			  </aside>
			  </section>
			  <section>
			  <h2>Questions you can answer</h2>
			  <p><ul>
			  <li>Can users get to full text?</li>
			  <li>How often do they get errors? What kind of errors?</li>
			  <li>Should you be satisfied with your knowledge base (kb) vendor?</li>
			  <li>Do you have enough full text in your collection?</li>
			  <li>How often do your users need ILL?</li>
			  <li>Are you teaching users what they need to know to successfully access e-resources?</li>
			  </ul></p>
			  <aside class="notes">
			  <p>Lessons learned from availability studies pertain to other areas of librarianship besides e-resources troubleshooting. You can use study results as evidence to address a variety of questions. </p>
			  </aside>
			  </section>
			  <section>
			  <h2>Availability studies</h2>
			  <p>4 searches x 10 disciplines x 10 results = <strong>400 item sample</strong></p>
			  <p>(estimates overall population with 98% confidence, +/- 5% error)</p>
			  <p><img data-src="img/availability-spreadsheet.png" alt="Screen shot of the availability survey spreadsheet" /></p>
			  <aside class="notes">
			  <p>I created a sample of 400 items by creating database searches based on actual reference questions from our ref desk statistics. I ran 4 searches in each of 10 A&I databases for different disciplines spanning the humanities, social sciences and sciences. I tried to obtain the full text of the first 10 results of each search, recording the results in a spreadsheet. All items are either locally available online, locally available in print, or available from other libraries through ILL. Errors are recorded in the six categories mentioned previously. Using statistical techniques for sampling of binomial (yes/no) variables, I determined that I can be 98% confident that my sample size of 400 items will estimate the population of all items in the library’s collection within +/- 5% margin of error.</p>
			  </aside>
			  </section>
			  <section>
			  <h2>Datasets</h2>
			  <p>2012 dataset at (broken link)</p>
			  <p>2013 dataset at (broken link)</p>
			  <aside class="notes">
			  <p>So far I have conducted three availability studies at Armacost Library. I conducted a pilot project to determine the sample size, and then the initial study of 400 items which I reported on last year at Research Day. After troubleshooting (discussed later) I replicated the same study again in 2013 to see if the local availability and error rates had changed. This was an important step to see if the availability study really had been effective at helping me discover problems. </p>
			  </aside>
			  </section>
			  <section>
			  <h2>Results</h2>
			  <p><img data-src="img/availability-success-results.png" alt="Bar graph comparing 2012 and 2013 results" /></p>
			  <aside class="notes">
			  <p>A comparison of my results from the two studies shows that the proportion of items locally available from Armacost Library’s print and online collections stayed the same, but the error rate dropped from 38% to 13%. Is this significant? </p>
			  </aside>
			  </section>
			  <section>
			  <h2>Z test results</h2>
			  <p><img data-src="img/availability-success-zresults.png" alt="Z test results plotted against a normal curve" /></p>
			  <aside class="notes">
			  <p>Used <i>100 Statistical Tests</i> by Gopal Kanji to determine that I could use the z test for difference of two proportions to see if the change in error rates was significant. The test looks at how much my results differ from the result we would expect if there really was no change in error rates. Z > 1.645 is statistically significant. The reduction in overall error rate is a VERY strong result so I can easily reject H0. No significant change in local availability rate so I would conclude that the troubleshooting steps I took did not make more items locally available. I also used Z tests to examine the change in error rates for each category of error to try to understand why I was or was not effective at fixing each category of error. </p>
			  </aside>
			  </section>
			  <section>
			  <h2>Troubleshooting proxy errors</h2>
			  <p><img data-src="img/availability-success-results-proxy.png" alt="Z test results for proxy errors" /></p>
			  <aside class="notes">
			  <p>Proxy errors generally meant the e-journal’s domain was missing from the proxy forward table. Usually these were e-journals with a unique domain. The solution, adding the domain to the forward table, only took a few minutes and was entirely under my control. I had a statistically significant impact on the proxy error rate. </p>
			  </aside>
			  </section>
			  <section>
			  <h2>Troubleshooting source errors</h2>
			  <p><img data-src="img/availability-success-results-source.png" alt="Z test results for source errors" /></p>
			  <aside class="notes">
			  <p>Source errors usually involve OpenURL metadata in the A&I database. There were a lot of these errors (8% of all items in my sample!) Sometimes very difficult to identify because the problem might not manifest until linking into a target database or placing an ILLIAD request. You have to look carefully at the OpenURL string before and after it enters the link resolver to determine if you have a source error. Source errors are also difficult to troubleshoot. I contact the database vendor each time I find an error, and they often need to forward the problem to a different unit, which may need to ask another party to fix the problem. It’s interesting to notice a trend in the 2013 study. In some situations where a library subscribes to both a full text and a citation-only database from the same publisher indexing the same item, the database inserts the PDF from the full text record into the record for the abstracting database, thus bypassing OpenURL linking entirely – and rendering source metadata errors a moot point.</p>
			  </aside>
			  </section>
			  <section>
			  <h2>Troubleshooting KB errors</h2>
			  <p><img data-src="img/availability-success-results-kb.png" alt="Z test results for knowledge base errors" /></p>
			  <aside class="notes">
			  <p>Knowledge base errors usually involve collections that do not resolve down to the article level. In my experience students often assume the item is not available when they are not taken directly to the article so I count this situation as an error that needs to be improved (cf. Trainor and Price 2010 who also categorized browsing for results as an error)  The collections in our knowledgebase for “Freely Available” journals are not centrally managed and only resolve to the title level, so I couldn’t affect the overall error rate.</p>
			  </aside>
			  </section>
			  <section>
			  <h2>Troubleshooting resolver errors</h2>
			  <p><img data-src="img/availability-success-results-resolver.png" alt="Z test results for link resolver errors" /></p>
			  <aside class="notes">
			  <p>Link resolver errors involve problems with the logic used to match items. For example, linking from the resolver result screen to the library catalog by title may match on multiple items or an incorrect item. We fixed problems by adding a prominent link to match on ISSN or ISBN instead. </p>
			  </aside>
			  </section>
			  <section>
			  <h2>Troubleshooting target errors</h2>
			  <p><img data-src="img/availability-success-results-target.png" alt="Z test results for target errors" /></p>
			  <aside class="notes">
			  <p>Target errors could occur because the full text database being linked into does not have the PDF loaded on their site or the site was down when I was testing it. Database vendors improved this category of problems through website redesigns, and by overlaying full text records as mentioned earlier.</p>
			  </aside>
			  </section>
			  <section>
			  <h2>Troubleshooting ILLIAD errors</h2>
			  <p><img data-src="img/availability-success-results-illiad.png" alt="Z test results for ILLIAD errors" /></p>
			  <aside class="notes">
			  <p>ILLIAD errors improved because OCLC upgraded ILLIAD to support Unicode, and because our web librarian created request forms for more item types and edited the Customization Manager to match OpenURL metadata with the correct field in the correct request form. </p>
			  </aside>
			  </section>
			  <section>
			  <h2>Item types in the sample</h2>
			  <p><img data-src="img/availability-success-item-types.png" alt="Pie chart of item types" /></p>
			  <aside class="notes">
			  <p>Some interesting facts about the samples: 75% articles, 11% gray literature (proceedings and dissertations, ILL only)</p>
			  </aside>
			  </section>
			  <section>
			  <h2>Error rate by discipline</h2>
			  <p><img data-src="img/availability-success-disciplines.png" alt="Bar graph of discipline error rates" /></p>
			  <aside class="notes">
			  <p>Disciplines where the database indexed a lot of gray literature (Music, English, PsycInfo, Math) had very high ILL error rates. Disciplines with article-dominant databases (History, Economics) had fewer errors and more locally available items. Philosophy had an unusually large number of source errors in 2013 study. </p>
			  </aside>
			  </section>
			  <section>
			  <h2>Only 2 of 5 items at the library</h2>
			  <p><img data-src="img/availability-success-items-library.png" alt="bar graph comparing availability in 2012 and 2013 studies" /></p>
			  <aside class="notes">
			  <p>Most surprising result for me was consistent 40% local availability (irrespective of whether an item produced an error or not). Research libraries often report about 60% local availability (Nisonger 2007). This means that 3 out of every 5 items in our A&I databases are not in our library collections. </p>
			  </aside>
			  </section>
			  <section>
			  <h2>Discussion</h2>
			  <p><ul>
			  <li>Availability studies can help you make a statistically significant reduction in erorrs</li>
			  <li>Supports evidence-based librarianship</li>
			  <ul><li>Advocate w/ vendors to improve metadata chain</li>
			  <li>More full text resources?</li>
			  <li>More ILL requests?</li>
			  <li>Should first years avoid A&I databases?</li>
			  <li>Do discovery services help or hurt?</li>
			  </ul>
			  </ul>
			  </p>
			  <aside class="notes">
			  <p>Comparing the 2012 and 2013 studies demonstrates that availability studies can help a library make a very statistically significant reduction in electronic resource errors. </p>
			  <p>Availability studies are a technique that lends well to evidence based librarianship and can impact decision making in a variety of areas:</p>
			  <p><ul>
			  <li>Advocating with vendors to improve the quality of metadata. If source errors are 8% of our A&I indexing databases, that’s too many errors to fix one at a time.</li>
			  <li>Regarding the 40% local issue, we could respond in a variety of ways. We could change collection development to prioritize acquiring full text, we could ramp up ILL services, or we could scaffold library instruction to direct students to full text resources until they need to do upper division research in a major, and are able to evaluate whether an unavailable resource really meets their research needs.</li>
			  <li>Need more information from other libraries on whether discovery services help or hurt? Are different knowledge bases more or less error prone? </li>
			  </ul>
			  </p>
			  </aside>
			  </section>
			  <section>
			  <h2>Next steps</h2>
			  <p><ul>
			  <li>Conduct availability studies at other libraries</li>
			  <li>Availability study with Redlands students</li>
			  <li>What are your e-resource experiences?</li>
			  </ul>
			  </p>
			  <aside class="notes">
			  <p>Natural next steps for this research are to conduct further availability studies at other libraries of different sizes, serving various user populations. I spent 25 minutes per search (16 hours total for my 400 item sample) but you can save time by using a 100 item sample and still get 81% confidence.  I am also in the process of conducting an availability study with Redlands students where I have them test access to the resources, hoping to present preliminary findings at NASIG in May.  If you are viewing this presentation online and have questions for me or are interested in sharing your e-resource availability/error experiences, write to me at (dead email). I’d love to hear from you!</p>
			  </aside>
			  </section>
			  <section>
			  <h2>Further reading</h2>
			  <p><ul>
			  <li>Kanji, Gopal K. 2006. 100 Statistical Tests. 3rd ed. Thousand Oaks: Sage Publications.</li>
			  <li>Mann, Sanjeet. 2013. “Measuring Electronic Resource Availability” presented at the SCELC Research Day, March 5, Loyola Marymount University. (url)</li>
			  <li>Nisonger, Thomas E. 2007. “A Review and Analysis of Library Availability Studies.” Library Resources & Technical Services 51 (1): 30–49.</li>
			  <li>Trainor, Cindi, and Jason Price. 2010. Rethinking Library Linking: Breathing New Life into OpenURL. Vol. 46. Library Technology Reports 7. Chicago: American Library Association.</li>
			  </ul>
			  </p>
			  </aside>
			  </section>






			  

			  


			
			</div>
		</div>

		<script src="dist/reveal.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
			    hash: true,

				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes ]
			});
		</script>
	</body>
</html>
