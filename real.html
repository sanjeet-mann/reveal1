<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Real Availability</title>

		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/orange.css">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css">
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
			  <section>
			    <h1>Why can't students get the sources they need?</h1>
				<hr>
				<h2>Results from a real availability study</h2>
			    <p>Sanjeet Mann<br />
			      Arts & Electronic Resources Librarian, University of Redlands<br />
			      29th Annual NASIG Conference, Fort Worth, Texas<br />
			      May 2, 2014</p>
			    <aside class="notes">
			      <p>This is the online, archived version of my presentation given May 2, 2014 at the 29th NASIG Conference in Fort Worth.</p>
				  <p>Systems and e-resource librarians have been actively involved in troubleshooting access errors since the early days of electronic resources in libraries. However, all too often, the personnel with the technical expertise to fix problems are not aware of a problem until someone else (a colleague or a patron) brings it to their attention. Even a basic sense of how frequently access problems occur, or what proportion of resources are affected, can be difficult to estimate. Availability studies are a systems analysis-based research method that librarians can use to estimate the proportion of items in a library collection which are available to users or affected by systems errors. I have conducted multiple electronic resource availability studies at the University of Redlands (Mann 2012, 2013). This presentation describes modifications that I made to my basic availability study method, in order to more accurately understand why undergraduate students at the University of Redlands cannot obtain the full text of electronic resources. </p>
			      </aside>
			  </section>
			  <section>
			  <h2>How do you get to full text?</h2>
			  <div style="float:left;"><h3>Librarian</h3>
			  <p><img data-src="img/availability-librarian.png" alt="Image of Sanjeet Mann" caption="Image credit: Paige Mann" /></p>
			  </div>
			  <div style="float:right;"><h3>Student</h3>
			  <p><img data-src="img/availability-student.png" alt="Stock photo of UOR students" caption="Image credit: Carlos Puma" /></p>
			  </div>
			  <aside class="notes">
			  <p>There are significant differences in how librarians and students go about obtaining full text, particularly if they come across a problem. In my live presentation, I demonstrated this by comparing how I would access a problematic article with a screen capture of a student actually seeking the same article. </p>
			  </aside>
			  </section>
			  <section>
			  <h2>Research methods</h2>
			  <div style="float:left;"><h3>Technical errors = availability study</h3>
			  <p><ul>
			  <li>Quantitative</li>
			  <li>Large sample for statistical validity</li>
			  <li>Researcher tests access ("simulated availability")</li>
			  </ul></p>
			  </div>
			  <div style="float:right;"><h3>Human interaction = usability study</h3>
			  <p><ul>
			  <li>Qualitative</li>
			  <li>5-7 users</li>
			  <li>Researcher observes library users ("real availability")</li>
			  </ul></p>
			  </div>
			  <aside class="notes">
			  <p>There are a couple of different methods that librarians can use to answer the question of why users cannot obtain the full text of items. Availability studies are useful at identifying technical or systems errors. This quantitative research method involves testing access to a statistically valid sample of items to estimate the availability or error rate in the entire library collection. Typically, librarians test access to the sample (“simulated” availability studies as opposed to “real” studies in which a user generates the sample) Task-based usability studies are useful at identifying points in a process (like obtaining full text) that are susceptible to human error. Task protocols produce qualitative data and may involve as few as 5-7 library users who obtain items themselves under librarian observation. I have conducted availability studies at University of Redlands which have helped me identify and reduce the number of system errors impacting electronic resources (Mann 2013). However, in order to accurately understand the reasons why actual library users cannot obtain items, I needed to modify my availability study technique to incorporate some aspects of usability testing. </p>
			  </aside>
			  </section>
			  <section>
			  <h2>My methodology</h2>
			  <p><ul>
			  <li>Cognitive walkthrough</li>
			  <li>7 students x 2 searches x 10 results = 142 interactions</li>
			  <li>Jing screen capture software</li>
			  <li>Demographic survey and results spreadsheet</li>
			  </ul></p>
			  <aside class="notes">
			  <p>For this study, I began by conducting a cognitive walkthrough to identify the most efficient way to test full text access to a citation in an A&I database:</p>
			  <p><ol>
			  <li>Click OpenURL icon (or click PDF icon if full text is “overlaid” into the A&I record)</li>
			  <li>Check result screen for online access.</li> 
			  <li>Click “Get Article” link if it is available, click “Get Journal” link and browse to full text if not.</li>
			  <li>If not available online, check library catalog for print access. Click “Find Journal by ISSN” and check holdings if there is a match.</li>
			  <li>If not available in print or online, request an ILL copy by clicking “Submit an Interlibrary Loan request” (I could have included a Google Scholar search here)</li>
			  </ol>
			  </p>
			  <p>I got IRB approval during Fall 2013 semester and recruited 7 students in the Spring 2014 semester. I assigned each student two searches in a given A&I database and had them test access to the first 10 search results from each search. This yielded 142 interactions between students and items. I asked students to use Jing screen capture software to record their activity. I also had them complete a short demographic survey asking about their year in college, level of information literacy (operationalized as their confidence with research measured on a 5 point Likert scale and a yes/no question asking if they had library instruction) and familiarity with the assigned database. Students had all received at least one library instruction session, rated themselves on a 3 or 4 in terms of confidence, but were not familiar with the database they were assigned. My data set from this study included the demographic surveys and about 3 hours of screen capture video. I reviewed each of the videos and filled out a results spreadsheet, which is a publicly available Google Spreadsheet.</p>
			</aside>
			</section>
			<section>
			<h2>Results</h2>
			<p><img data-src="img/availability-real-results.png" alt="Results pie chart" /></p>
			<aside class="notes">
			<p>Each student’s interaction with a citation resulted in one of four possible outcomes. They either:</p>
			<p><ol>
			<li>Downloaded the full text from Armacost Library databases (29% of interactions)</li>
			<li>Determined that the full text was available from Armacost Library’s physical collection (students did not actually try to get the item from the shelf) (3%)</li>
			<li>Determined that the full text must be requested from another library through ILL (students did not actually place requests) (43%)</li>
			<li>Failed to obtain the full text altogether (25%)</li>
			</ol>
			</p>
			</aside>
			</section>


			
			</div>
		</div>

		<script src="dist/reveal.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
			    hash: true,

				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes ]
			});
		</script>
	</body>
</html>
