<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Real Availability</title>

		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/orange.css">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css">
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
			  <section>
			    <h1 style="text-align:left;">Why can't students get the sources they need?</h1>
				<hr>
				<h2 style="text-align:left;"  >Results from a real availability study</h2>
			    <p>Sanjeet Mann<br />
			      Arts & Electronic Resources Librarian, University of Redlands<br />
			      29th Annual NASIG Conference, Fort Worth, Texas<br />
			      May 2, 2014</p>
			    <aside class="notes">
			      <p>This is the online, archived version of my presentation given May 2, 2014 at the 29th NASIG Conference in Fort Worth.</p>
				  <p>Systems and e-resource librarians have been actively involved in troubleshooting access errors since the early days of electronic resources in libraries. However, all too often, the personnel with the technical expertise to fix problems are not aware of a problem until someone else (a colleague or a patron) brings it to their attention. Even a basic sense of how frequently access problems occur, or what proportion of resources are affected, can be difficult to estimate. Availability studies are a systems analysis-based research method that librarians can use to estimate the proportion of items in a library collection which are available to users or affected by systems errors. I have conducted multiple electronic resource availability studies at the University of Redlands (Mann 2012, 2013). This presentation describes modifications that I made to my basic availability study method, in order to more accurately understand why undergraduate students at the University of Redlands cannot obtain the full text of electronic resources. </p>
			      </aside>
			  </section>
			  <section>
			  <h2>How do you get to full text?</h2>
			  <div style="float:left;"><h3>Librarian</h3>
			  <p><img data-src="img/availability-librarian.png" alt="Image of Sanjeet Mann" caption="Image credit: Paige Mann" /></p>
			  </div>
			  <div style="float:right;"><h3>Student</h3>
			  <p><img data-src="img/availability-student.png" alt="Stock photo of UOR students" caption="Image credit: Carlos Puma" /></p>
			  </div>
			  <aside class="notes">
			  <p>There are significant differences in how librarians and students go about obtaining full text, particularly if they come across a problem. In my live presentation, I demonstrated this by comparing how I would access a problematic article with a screen capture of a student actually seeking the same article. </p>
			  </aside>
			  </section>
			  <section>
			  <h2>Research methods</h2>
			  <div style="float:left; max-width:49%"><h3>Technical errors = <br />availability study</h3>
			  <p><ul>
			  <li>Quantitative</li>
			  <li>Large sample for statistical validity</li>
			  <li>Researcher tests access ("simulated availability")</li>
			  </ul></p>
			  </div>
			  <div style="float:right; max-width:49%"><h3>Human interaction = <br />usability study</h3>
			  <p><ul>
			  <li>Qualitative</li>
			  <li>5-7 users</li>
			  <li>Researcher observes library users ("real availability")</li>
			  </ul></p>
			  </div>
			  <aside class="notes">
			  <p>There are a couple of different methods that librarians can use to answer the question of why users cannot obtain the full text of items. Availability studies are useful at identifying technical or systems errors. This quantitative research method involves testing access to a statistically valid sample of items to estimate the availability or error rate in the entire library collection. Typically, librarians test access to the sample (“simulated” availability studies as opposed to “real” studies in which a user generates the sample) Task-based usability studies are useful at identifying points in a process (like obtaining full text) that are susceptible to human error. Task protocols produce qualitative data and may involve as few as 5-7 library users who obtain items themselves under librarian observation. I have conducted availability studies at University of Redlands which have helped me identify and reduce the number of system errors impacting electronic resources (Mann 2013). However, in order to accurately understand the reasons why actual library users cannot obtain items, I needed to modify my availability study technique to incorporate some aspects of usability testing. </p>
			  </aside>
			  </section>
			  <section>
			  <h2>My methodology</h2>
			  <p><ul>
			  <li>Cognitive walkthrough</li>
			  <li>7 students x 2 searches x 10 results = 142 interactions</li>
			  <li>Jing screen capture software</li>
			  <li>Demographic survey and results spreadsheet</li>
			  </ul></p>
			  <aside class="notes">
			  <p>For this study, I began by conducting a cognitive walkthrough to identify the most efficient way to test full text access to a citation in an A&I database:</p>
			  <p><ol>
			  <li>Click OpenURL icon (or click PDF icon if full text is “overlaid” into the A&I record)</li>
			  <li>Check result screen for online access.</li> 
			  <li>Click “Get Article” link if it is available, click “Get Journal” link and browse to full text if not.</li>
			  <li>If not available online, check library catalog for print access. Click “Find Journal by ISSN” and check holdings if there is a match.</li>
			  <li>If not available in print or online, request an ILL copy by clicking “Submit an Interlibrary Loan request” (I could have included a Google Scholar search here)</li>
			  </ol>
			  </p>
			  <p>I got IRB approval during Fall 2013 semester and recruited 7 students in the Spring 2014 semester. I assigned each student two searches in a given A&I database and had them test access to the first 10 search results from each search. This yielded 142 interactions between students and items. I asked students to use Jing screen capture software to record their activity. I also had them complete a short demographic survey asking about their year in college, level of information literacy (operationalized as their confidence with research measured on a 5 point Likert scale and a yes/no question asking if they had library instruction) and familiarity with the assigned database. Students had all received at least one library instruction session, rated themselves on a 3 or 4 in terms of confidence, but were not familiar with the database they were assigned. My data set from this study included the demographic surveys and about 3 hours of screen capture video. I reviewed each of the videos and filled out a results spreadsheet, which is a publicly available Google Spreadsheet.</p>
			</aside>
			</section>
			<section>
			<h2>Results</h2>
			<p><img data-src="img/availability-real-results.png" alt="Results pie chart" /></p>
			<aside class="notes">
			<p>Each student’s interaction with a citation resulted in one of four possible outcomes. They either:</p>
			<p><ol>
			<li>Downloaded the full text from Armacost Library databases (29% of interactions)</li>
			<li>Determined that the full text was available from Armacost Library’s physical collection (students did not actually try to get the item from the shelf) (3%)</li>
			<li>Determined that the full text must be requested from another library through ILL (students did not actually place requests) (43%)</li>
			<li>Failed to obtain the full text altogether (25%)</li>
			</ol>
			</p>
			</aside>
			</section>
			<section>
			<h2>Errors</h2>
			<p><img data-src="img/availability-real-errors.png" alt="Venn diagram of user errors overlapping with system errors" /></p>
			<aside class="notes">
			<p>Independent of whether students were able to obtain the desired item online, in print or by ILL, I also tracked system and user errors by reviewing the screen capture recordings, and comparing them to my cognitive walkthrough. I was looking for examples of outright system errors (error messages, missing fields in system-generated forms, etc.) and likely human errors (students repeatedly running into a dead end, entering invalid or incorrect information into a search field, or otherwise demonstrating a significant lack of understanding about what they were doing) I found a nearly equal proportion of interactions involving “system” and “user” errors. In many of these cases, the student was able to overcome the error in order to create an ILL request or obtain the full text from the library’s collection. This confirms a suspicion that I had (but couldn’t verify) during my earlier simulated studies, that not all e-resource errors are equally serious. </p>
			</aside>
			</section>
			<section>
			<h2>Severe system errors</h2>
			<p><ul>
			<li>A&I database has no OpenURL link</li>
			<li>Target database refuses the OpenURL</li>
			<li>A&I database has bad/missing metadata</li>
			<ul><li>Can info be found in Google?</li></ul>
			<li>Knowledge base doesn't offer article-level linking</li>
			<ul><li>Is student willing to browse?</li></ul>
			</ul></p>
			<aside class="notes">
			<p>In analyzing situations where a user was not able to overcome the error, I found some patterns in the types of errors that would most likely prevent access. Failure to display an OpenURL link for some item types (e.g. gray literature) and refusing to accept the OpenURL link are serious problems which overpowered multiple students trying to test those items. Students were willing to search Google to find metadata (such as rft.date) missing from the A&I record, or try to browse an e-journal site to find the full text of an article when the knowledge base only provided a title level link. However, in several cases, the student became confused during the results of this browsing or searching. System errors add additional complexity to student interactions with e-resources – perhaps just enough complexity to overwhelm them. </p>
			</aside>
			</section>
			<section>
			<h2>Student encounters a system error</h2>
			<p><img data-src="img/availability-real-system-error.png" alt="Example of a system error" /></p>
			<aside class="notes">
			<p>Here is an example of an interaction containing both a system and human interaction error. The student gets an error message when she clicks the “Get Article” link, and is greeted by an unfamiliar search box when she clicks the “Get Journal link”. She tries Google Scholar and copies in the article title when she realizes the search does not auto-populate. She overlooks a full text link from an institutional repository and instead clicks on a link pointing to the publisher’s website, where she hits a paywall and gives up. </p>
			</aside>
			</section>
			<section>
			<h2>Severe human errors</h2>
			<p><ul>
			<li>Didn't test link</li>
			<li>Used system incorrectly</li>
			<li>Overlooked important information</li>
			<li>Got frustrated and gave up</li>
			</ul></p>
			<aside class="notes">
			<p>I also noticed patterns in the types of human errors that prevented students from obtaining items. Sometimes students did not test an item, perhaps because they overlooked it in the list of search results or lost their place after returning to the search result screen from another tab. </p>
			<p>I observed several students using library search tools in ways that would be unlikely to give them the desired result – for example, running an author search in the catalog in firstname lastname format or searching the title of a dissertation in our Serials Solutions journal search box. </p>
			<p>Often students seemed to overlook important information. For example, we just saw one student overlook the link to full text from an institutional repository. Another student arrived at a link resolver screen with a “Get Article” link but instead searched for the item in the print collection (it wasn’t there) and then requested ILL.</p>
			<p>Finally, many students gave up before they obtained an item in full text or even opened the ILL request form. The level of persistence varied greatly from student to student. Since they were participating in a research study and asked specifically to obtain the full text of all 10 items, I expect that the students recruited for this study demonstrated far greater persistence than we would expect to find in normal student interaction with e-resources. </p>
			</aside>
			</section>
			<section>
			<h2>Student experiences user error</h2>
			<p><img data-src="img/availability-real-user-error.png" alt="Exampleof a user error" /></p>
			<aside class="notes">
			<p>The student tried to obtain a dissertation but no dissertations are available electronically through our collection. After some difficulty he pasted his search into Google Scholar and arrived at a UMI paywall, which mentioned that libraries subscribing to Proquest Dissertations & Theses Full Text would have access to the item. The student searched for “proquest” in the Serials Solutions journal search box and found results for two unrelated Proquest databases. He tried searching the title in one of those databases and got an error message. At this point he returned to the link resolver result screen and submitted an ILL request.</p>
			</aside>
			</section>
			<section>
			<h2>Conceptual model</h2>
			<p><img data-src="img/availability-real-conceptual-model.png" alt="Conceptual model combining system and user errors" /></p>
			<aside class="notes">
			<p>Here is a new conceptual model combining system and user availability errors</p>
			</aside>
			</section>
			<section>
			<h2>Questions availability studies can address</h2>
			<p><ul>
			<li>How often do errors occur? Should we be satisfied with our technical infrastructure? (systems)</li>
			<li>How often do users need ILL? (interlibrary loan)</li>
			<li>Do we have enough full text in the collection? (collection development)</li>
			<li>Are we teaching users what they need to be successful at obtaining electronic resources? (instruction)</li>
			</ul></p>
			<aside class="notes">
			<p>This research project, and availability studies in general, are obviously useful for electronic resource troubleshooting and web design. With information from this research project I will be able to train staff to perform troubleshooting and proactively detect e-resource errors. Information gained from an availability study can also address common questions in other areas of library operations. </p>
			</aside>
			</section>
			<section>
			<h2>For discussion at University of Redlands</h2>
			<p><ul>
			<li>86.5% availability rate in 2013 simulated study</li>
			<ul><li>Common problems = source metadata, KB support for OA titles</li></ul>
			<li>26.8% local availability (research libraries average 60%)</li>
			<ul><li>2 of 3 items not held in local collection in 2013 simulated study</li>
			<li>43% of interactions result in ILL in 2014 real study</li></ul>
			<li>Threshold concepts vs. search/retrieval mechanics</li>
			<ul><li>How do you teach students to be thoughtful, resilient searchers?</li></ul>
			</ul>
			<aside class="notes">
			<p>I plan to discuss some particular findings with my colleagues at the University of Redlands. In the present study many items lacked one or more fields of OpenURL metadata and many OA items were not represented in Serials Solutions. 86.5% of items were available through the library collection or ILL in my 2013 availability study. Most of the errors that made items unavailable were metadata or knowledge base related. These findings are probably not a surprise to anyone who attended this year’s NASIG sessions on managing open access resources, or on NISO standards such as KBART and IOTA aimed at addressing OpenURL and knowledge base problems. The persistence of these types of errors demonstrates that these issues need continued attention from librarians, publishers and others involved in the metadata supply chain. 26.8% of user interactions in this study resulted in items that could be obtained from the library’s physical or electronic collection. This is below the 35% local availability rate that I observed in my 2013 simulated study, and far below the 60% that research libraries like UIUC have reported. We need to discuss the role of A&I databases and “non-local” items in the library collection. Is it a problem that we cannot supply 2 of 3 items that our students discover in our A&I databases from our own collection? How does the fact that 43% of interactions resulted in an ILL request impact our ILL operations? Does this suggest a potential unmet demand for ILL? Some libraries that have implemented discovery services (Redlands has not) have reported significant increases in ILL requests as a result of the increased discoverability of items. If more items are available through ILL than the library’s own collection, perhaps we need to reconceive of collections in a radically different manner, for example, in terms of time to supply the full text rather than the “proximity” of the item to the user.  Finally, the high volume of interactions involving students skipping, misunderstanding or overlooking vital information suggests that we need to continue to teach information retrieval to students. Even as ALA is revising the ACRL standards to focus on threshold concepts (the irrevocable, “big picture” learning that transforms students’ worldview) students are still having a very hard time with something as “simple” as getting the full text of an article they want to cite for a paper. A particular challenge facing all librarians with a role in instruction (whether we specialize in ER or not) is how to motivate students to plan their search strategy carefully, pay attention to signals they are getting from the search tools, and have the patience and ingenuity to find a workaround for problems that arise.  </p>
			</aside>
			</section>
			<section>
			<h2>What would I do differently?</h2>
			<p><ul>
			<li>Larger, personalized incentives</li>
			<li>Simplify research design</li>
			<li>Cognitive walkthrough as group activity</li>
			<li>Jing + Camtasia Studio worked ok</li>
			</ul></p>
			<aside class="notes">
			<p>Overall, I found that combining the availability study with techniques borrowed from usability research helped me examine both system and human factors affecting e-resource availability, and get a richer perspective on what prevents students from obtaining items they need. I encourage readers of this presentation to adapt my methodology to learn about their own libraries and improve access for their users. I close with some lessons I learned which could help future “real” availability studies to be more successful.  I was hoping to get more than 7 participants, but I had trouble recruiting because my incentive (a $50 raffle) was not personalized. I would recommend budgeting for individual incentives in the future.  My research design was more complex than it needed to be. I assigned students their two searches from a pool of six searches taken from my original study (so I could compare the results). I wanted multiple students to be able to test each search in the pool, but I didn’t get enough students. I should have had each student run the same search until I built up 5 students testing the same search, before moving on to a different search.  I conducted the cognitive walkthrough and the analysis of the screen capture recordings myself. My results would be more reliable if I did these as a group activity. These activities can be a very interesting conversation starter and help your library staff really understand the challenges facing students. I used the free Jing software, which recorded in five minute increments. Every five minutes students would have to save the existing file to the desktop and create a new screen capture. This did not pose any problems for the seven recruited students. If you use the paid Snagit or another screen capture program you would not face this limitation. I also used an ITS laptop with Camtasia studio to stitch together the captures into one video for each student, and iMovie to edit the short clip used in this presentation. </p>
			</aside>
			</section>
			<section>
			<h2>Further reading</h2>
			<p>Link to website</p>
			<p>Selective bibliography</p>
			<p>Presentation slides</p>
			<p>University of Redlands availability study datasets:</p>
			<p><ul>
			<li>2012 (simulated)</li>
			<li>2013 (simulated)</li>
			<li>2014 (real)</li>
			</ul></p>
			<aside class="notes">
			<p>My selective bibliography of availability literature, data sets from my three University of Redlands studies, and slides from this and other availability presentations I have conducted are all online at (broken link) Feel free to contact me at (dead email address) with any questions you have about this research project. </p>
			</aside>
			</section>


			



			
			</div>
		</div>

		<script src="dist/reveal.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
			    hash: true,

				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes ]
			});
		</script>
	</body>
</html>
